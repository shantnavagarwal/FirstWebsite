{
    "version": "https://jsonfeed.org/version/1",
    "title": "shantnavagarwal",
    "description": "",
    "home_page_url": "https://shantnavagarwal.github.io/Website",
    "feed_url": "https://shantnavagarwal.github.io/Website/feed.json",
    "user_comment": "",
    "icon": "https://shantnavagarwal.github.io/Website/media/website/Logo.png",
    "author": {
        "name": "Shantnav Agarwal"
    },
    "items": [
        {
            "id": "https://shantnavagarwal.github.io/Website/advanced-driver-assisstance-system.html",
            "url": "https://shantnavagarwal.github.io/Website/advanced-driver-assisstance-system.html",
            "title": "Advanced Driver Assistance System",
            "summary": "<p>While self driving car technology is a bit too ahead of us, we can still use its advanced technologies to make our roads safer! I and my friend Hitesh worked with Prof. Sudipto Mukherjee (IITD) on developing an ADAS. We focused on developing algorithms that use Computer Vision to extract useful data from videos and assist the driver in performing safe driving practices.</p>\n",
            "content_html": "<p>While self driving car technology is a bit too ahead of us, we can still use its advanced technologies to make our roads safer! I and my friend Hitesh worked with Prof. Sudipto Mukherjee (IITD) on developing an ADAS. We focused on developing algorithms that use Computer Vision to extract useful data from videos and assist the driver in performing safe driving practices.</p>\n\n<h2 id=\"mcetoc_1dpa6mevi0\">Our Equipment</h2>\n<p>Our objective was to install multiple cameras in a car, and process their data to advise the driver in real-time if required. We had the following equipment on hand:</p>\n<table style=\"width: 653px; height: 612px;\">\n<tbody>\n<tr>\n<td style=\"width: 82px;\">\n<p><span style=\"text-decoration: underline;\"><strong>S. No</strong></span></p>\n</td>\n<td style=\"width: 291px;\">\n<p><span style=\"text-decoration: underline;\"><strong>Name</strong></span></p>\n</td>\n<td style=\"width: 157.381px;\">\n<p><span style=\"text-decoration: underline;\"><strong>Model Number</strong></span></p>\n</td>\n<td style=\"width: 118.619px;\">\n<p><span style=\"text-decoration: underline;\"><strong>Quantity</strong></span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 82px;\">\n<p>1.      </p>\n</td>\n<td style=\"width: 291px;\">\n<p>Basler IP Fixed Box Cameras</p>\n</td>\n<td style=\"width: 157.381px;\">\n<p>BIP2-1300C-DN</p>\n</td>\n<td style=\"width: 118.619px;\">\n<p>6 Units</p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 82px;\">\n<p>2.      </p>\n</td>\n<td style=\"width: 291px;\">\n<p>D-Link Network Box</p>\n</td>\n<td style=\"width: 157.381px;\">\n<p>DES-1210-08P</p>\n</td>\n<td style=\"width: 118.619px;\">\n<p>1 Unit</p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 82px;\">\n<p>3.      </p>\n</td>\n<td style=\"width: 291px;\">\n<p>Vehicle</p>\n</td>\n<td style=\"width: 157.381px;\">\n<p>Nissan X-Trail</p>\n</td>\n<td style=\"width: 118.619px;\">\n<p>1 Unit</p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 82px;\">\n<p>4.      </p>\n</td>\n<td style=\"width: 291px;\">\n<p>Matrix Computer</p>\n</td>\n<td style=\"width: 157.381px;\">\n<p>MXE-5401</p>\n</td>\n<td style=\"width: 118.619px;\">\n<p>1 Unit</p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 82px;\">\n<p>5.      </p>\n</td>\n<td style=\"width: 291px;\">\n<p>Micro-Controller</p>\n</td>\n<td style=\"width: 157.381px;\">\n<p>Arduino Uno</p>\n</td>\n<td style=\"width: 118.619px;\">\n<p>1 Unit</p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 82px;\">\n<p>6.      </p>\n</td>\n<td style=\"width: 291px;\">\n<p>Power Supply</p>\n</td>\n<td style=\"width: 157.381px;\">\n<p>1000 Watts</p>\n</td>\n<td style=\"width: 118.619px;\">\n<p>1 Unit</p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 82px;\">\n<p>7.      </p>\n</td>\n<td style=\"width: 291px;\">\n<p>Connecting Wires, Multi-meter etc</p>\n</td>\n<td style=\"width: 157.381px;\">\n<p> </p>\n</td>\n<td style=\"width: 118.619px;\">\n<p> </p>\n</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"mcetoc_1dpg3jint0\"> Process</h2>\n<p>We identified the 3 areas in which we needed to work to properly instrument the car. They were:</p>\n<ul>\n<li>Providing power to the computers and cameras</li>\n<li>Ensure that all the cameras are synchronised - We want them to take images simultaneously</li>\n<li>Damp the vibrations introduced in the camera due to the movement in the car\n<figure class=\"post__image post__image--right\" ><img src=\"https://shantnavagarwal.github.io/Website/media/posts/2/20181016_102810074_iOS.jpg\" alt=\"\" width=\"4032\" height=\"1000\">\n<figcaption >Wiring and inverter in the vehicle's boot</figcaption>\n</figure>\n</li>\n</ul>\n<p>The cameras, computer and monitor together needed about 350 watts of AC power. Considering efficiency losses, we needed a 12 volt DC to 220 volt AC converter with a rating of 500 watts. However, it was not possible to supply 500 watts over the existing electrical network in the car. We modified the cars internal wiring to provide enough juice to our devices.</p>\n<p>To syncronise the cameras, we developed a cycle in which:</p>\n<ol>\n<li>The camera captures a frame when its real-time electronic trigger is activated</li>\n<li>This trigger is activated via an Arduino Uno when requested by the computer.</li>\n<li>The computer initiates this request when it has finished processing the previous set of frames.</li>\n</ol>\n<p>Through this cycle, we were able to ensure that each camera captured its respective frames within 7ms of each other.</p>\n<figure class=\"post__image post__image--center\" ><img src=\"https://shantnavagarwal.github.io/Website/media/posts/2/IMG_0626.gif\" width=\"500\" height=\"281\">\n<figcaption >Stream from 4 cameras is being advaced one frame at a time in a clockwise fashion</figcaption>\n</figure>\n<p> </p>",
            "image": "https://shantnavagarwal.github.io/Website/media/posts/2/20181016_102911748_iOSNEW.png",
            "author": {
                "name": "Shantnav Agarwal"
            },
            "tags": [
            ],
            "date_published": "2019-11-10T12:52:09+05:30",
            "date_modified": "2019-11-15T18:46:47+05:30"
        },
        {
            "id": "https://shantnavagarwal.github.io/Website/traffic-sign-classifier.html",
            "url": "https://shantnavagarwal.github.io/Website/traffic-sign-classifier.html",
            "title": "CNN for detecting traffic signs",
            "summary": "Recognition of Traffic signs is one of the most crucial tasks while driving on the roads. This post provides an overview of the pipeline built by me to train a Convolutional Neural Net built by me to recognise traffic signs. I used the GTSRB dataset&hellip;",
            "content_html": "<p>Recognition of Traffic signs is one of the most crucial tasks while driving on the roads. This post provides an overview of the pipeline built by me to train a Convolutional Neural Net built by me to recognise traffic signs.</p>\n<h2 id=\"mcetoc_1dpa0i8v80\"><strong>Dataset summary &amp; distribution</strong></h2>\n<figure class=\"post__image post__image--left\" ><img src=\"https://shantnavagarwal.github.io/Website/media/posts/1/BarChart.png\" alt=\"Bar chart showing number of samples in training set for each label \" width=\"385\" height=\"252\">\n<figcaption >Bar chart shows number of samples in training data per label</figcaption>\n</figure>\n<p>I used the GTSRB dataset to train my network. It had almost 35000 images divided into 42 classes. As shown in the bar chart on the right, the number of datapoints for each class was highly skewed. To improve classification accuracy, I generated extra datapoints such that all labels had atleast 800 data points. To generate the extra datapoints, I scaled and warped the images with their parameters being sampled from a uniform distribution. A few examples of these generated datapoints is shown below.</p>\n<figure class=\"post__image post__image--wide\" ><img src=\"https://shantnavagarwal.github.io/Website/media/posts/1/Examples.png\" alt=\"\" width=\"855\" height=\"373\">\n<figcaption >\n<p>Examples of generated images</p>\n</figcaption>\n</figure>\n<h2 id=\"mcetoc_1dpa16rmp1\">Pre-processing the data</h2>\n<p>To keep the size of the network in check, I decided to use grayscale images instead of colour ones.  On observing the results, I realised that some images were simply too dark. I found it difficult to myself classify these traffic signs! Thus, to further enhace the contrast of these images, I used Contrast Limiting Adaptive Histogram Equalizer (CLAHE) on each image. This produced fantastic results as shown below.</p>\n<div class=\"gallery\"   >\n<figure class=\"gallery__item\"><a href=\"https://shantnavagarwal.github.io/Website/media/posts/1/gallery/Greyscale.png\" ><img src=\"https://shantnavagarwal.github.io/Website/media/posts/1/gallery/Greyscale-thumbnail.png\" alt=\"\" width=\"720\" height=\"317\"></a>\n<figcaption class=\"gallery-description\">Greyscale</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://shantnavagarwal.github.io/Website/media/posts/1/gallery/CLAHE.png\" ><img src=\"https://shantnavagarwal.github.io/Website/media/posts/1/gallery/CLAHE-thumbnail.png\" alt=\"\" width=\"720\" height=\"317\"></a>\n<figcaption class=\"gallery-description\">CLAHE</figcaption>\n</figure>\n</div>\n<p>Particularly impressive is the \"Men at Work\" sign (row 3, col 5) which was not comprehensible before the CLAHE filter was applied. Finally the images were standardized to between 0 and 1.</p>\n<h2 id=\"mcetoc_1dpa2lijd2\">Results</h2>\n<p>I was able to sucessfully train the model to deliver an accuracy of over 98% on the testing data.</p>\n<p> </p>\n<p> </p>",
            "image": "https://shantnavagarwal.github.io/Website/media/posts/1/TSC.jpeg",
            "author": {
                "name": "Shantnav Agarwal"
            },
            "tags": [
            ],
            "date_published": "2019-10-25T20:24:00+05:30",
            "date_modified": "2019-11-10T12:49:03+05:30"
        }
    ]
}
